{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Homework\n",
    "\n",
    "Group *May oh ness*\n",
    "\n",
    "Nick Halliwell, Aina Lopez, Yaroslav Marchuk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A skeleton class structure for documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "import math \n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from itertools import repeat\n",
    "\n",
    "\n",
    "class Corpus():\n",
    "    \n",
    "    \"\"\" \n",
    "    The Corpus class represents a document collection\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, doc_data, stopword_file, clean_length):\n",
    "        \"\"\"\n",
    "        Notice that the __init__ method is invoked everytime an object of the class\n",
    "        is instantiated\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        #Initialise documents by invoking the appropriate class\n",
    "        self.docs = [Document(doc[0], doc[1], doc[2]) for doc in doc_data] \n",
    "        \n",
    "        self.N = len(self.docs)\n",
    "        self.clean_length = clean_length\n",
    "        \n",
    "        #get a list of stopwords\n",
    "        self.create_stopwords(stopword_file, clean_length)\n",
    "        \n",
    "        #stopword removal, token cleaning and stemming to docs\n",
    "        self.clean_docs(2)\n",
    "        \n",
    "        #create vocabulary\n",
    "        self.corpus_tokens()\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \"\"\" \n",
    "        Applies stopword removal, token cleaning and stemming to docs\n",
    "        \"\"\"\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            doc.stem()        \n",
    "    \n",
    "    def create_stopwords(self, stopword_file, length):\n",
    "        \"\"\"\n",
    "        description: parses a file of stowords, removes words of length 'length' and \n",
    "        stems it\n",
    "        input: length: cutoff length for words\n",
    "               stopword_file: stopwords file to parse\n",
    "        \"\"\"\n",
    "        \n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        \n",
    "        self.stopwords = (np.array([PorterStemmer().stem(word) \n",
    "                                    for word in list(raw.splitlines()) if len(word) > length]))\n",
    "        \n",
    "     \n",
    "    def corpus_tokens(self):\n",
    "        \"\"\"\n",
    "        description: create a set of all all tokens or in other words a vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        #initialise an empty set\n",
    "        self.token_set = set()\n",
    "        for doc in self.docs:\n",
    "            self.token_set = self.token_set.union(doc.tokens) \n",
    "            \n",
    "    def document_term_matrix(self):\n",
    "        \"\"\"\n",
    "        description:  returns a D by V array of frequency counts\n",
    "        \"\"\"  \n",
    "        # subroutine: computes the counts of each vocabulary in the document\n",
    "        def counts(doc):\n",
    "            # initialize a matrix\n",
    "            term_mat = [0]*len(self.token_set)\n",
    "            for token in doc.tokens:\n",
    "                term_mat[list(self.token_set).index(token)] = term_mat[list(self.token_set).index(token)] + 1\n",
    "            return term_mat;\n",
    "            \n",
    "        self.doc_term_matrix = []\n",
    "        \n",
    "        for doc in self.docs:\n",
    "            self.doc_term_matrix.append([doc.pres + \" \" + doc.year, counts(doc)])\n",
    "\n",
    "\n",
    "      \n",
    "    def tf_idf(self):\n",
    "        \"\"\"\n",
    "        description:  returns a D by V array of tf-idf scores\n",
    "        \"\"\"\n",
    "        # Compute inverse document frequency \n",
    "        idf = [0]*len(self.token_set)\n",
    "        for token in self.token_set:\n",
    "            ind = 0\n",
    "            for doc in self.docs:\n",
    "                if token in doc.tokens:\n",
    "                    ind += 1 \n",
    "            idf[list(self.token_set).index(token)] = math.log(self.N/ind)\n",
    "        \n",
    "        # Create a subroutine that computes tf_idf for one document\n",
    "        def tfidf(doc):\n",
    "            term_mat = [0]*len(self.token_set)\n",
    "            for token in doc.tokens:\n",
    "                term_mat[list(self.token_set).index(token)] = term_mat[list(self.token_set).index(token)] + 1 \n",
    "        \n",
    "            for i,term in enumerate(term_mat):\n",
    "                if term != 0:\n",
    "                    term_mat[i] = (1 + math.log(term)) * idf[i]\n",
    "            return term_mat;\n",
    "        \n",
    "        #tf_idf\n",
    "        self.tf_idf_matrix = []\n",
    "        for doc in self.docs:\n",
    "            self.tf_idf_matrix.append([doc.pres + \" \" + doc.year, tfidf(doc)])\n",
    "            \n",
    "            \n",
    "        \n",
    "    def dict_rank(self, n, dictionary, token_repr):\n",
    "        \"\"\"\n",
    "        description:  returns the top n documents based on a given dictionary and represenation of tokens\n",
    "        \"\"\"\n",
    "        if token_repr == \"tf-idf\":\n",
    "            self.tf_idf()\n",
    "            representation = self.tf_idf_matrix\n",
    "            \n",
    "        if token_repr == \"doc-term\":\n",
    "            self.document_term_matrix()\n",
    "            representation = self.doc_term_matrix\n",
    "            \n",
    "        # Return top n docs based on dictionary given\n",
    "        score = []\n",
    "        x=self.token_set\n",
    "        x=list(x)\n",
    "        for token in x: \n",
    "            try:\n",
    "                score.append(dictionary[token])\n",
    "            except: \n",
    "                score.append(0)\n",
    "\n",
    "        # get a vector with all the scores in order\n",
    "        score=[int(x) for x in score]\n",
    "        rank = {}\n",
    "        elements=range(len(representation))\n",
    "   \n",
    "        for i in elements:\n",
    "            rank[representation[i][0]] = np.dot(score,representation[i][1])\n",
    "            \n",
    "        # Get sorted view of the keys.\n",
    "        s = sorted(rank, key=rank.get, reverse=True)[0:(n-1)]\n",
    "        \n",
    "        ranking = {}\n",
    "        for key in s:\n",
    "            ranking[key] =  rank[key]\n",
    "        \n",
    "        return ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Document():\n",
    "    \n",
    "    \"\"\" The Doc class rpresents a class of individul documents\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, speech_year, speech_pres, speech_text):\n",
    "        self.year = speech_year\n",
    "        self.pres = speech_pres\n",
    "        self.text = speech_text.lower()\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        description: strip out non-alpha tokens and tokens of length > 'length'\n",
    "        input: length: cut off length \n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Remove stopwords from tokens.\n",
    "        input: stopwords: a suitable list of stopwords\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.tokens = np.array([PorterStemmer().stem(t) for t in self.tokens])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Load the document and create the corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_text(textraw, regex):\n",
    "    \"\"\"takes raw string and performs two operations\n",
    "    1. Breaks text into a list of speech, president and speech\n",
    "    2. breaks speech into paragraphs\n",
    "    \"\"\"\n",
    "    prs_yr_spch_reg = re.compile(regex, re.MULTILINE|re.DOTALL)\n",
    "    \n",
    "    #Each tuple contains the year, last ane of the president and the speech text\n",
    "    prs_yr_spch = prs_yr_spch_reg.findall(textraw)\n",
    "    \n",
    "    #convert immutabe tuple to mutable list\n",
    "    prs_yr_spch = [list(tup) for tup in prs_yr_spch]\n",
    "    \n",
    "    for i in range(len(prs_yr_spch)):\n",
    "        prs_yr_spch[i][2] = prs_yr_spch[i][2].replace('\\n', '')\n",
    "    \n",
    "    #sort\n",
    "    prs_yr_spch.sort()\n",
    "    \n",
    "    return(prs_yr_spch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text = open(\"/Users/ainalopez/Downloads/text_mining-master/data/pres_speech/sou_all copy.txt\", 'r').read()\n",
    "text = open(\"/home/yaroslav/Projects/text_mining/data/pres_speech/sou_all.txt\", 'r').read()\n",
    "regex = \"_(\\d{4}).*?_[a-zA-Z]+.*?_[a-zA-Z]+.*?_([a-zA-Z]+)_\\*+(\\\\n{2}.*?)\\\\n{3}\"\n",
    "pres_speech_list = parse_text(text, regex)\n",
    "\n",
    "#Instantite the corpus class\n",
    "corpus = Corpus(pres_speech_list, '/home/yaroslav/Projects/text_mining/data/stopwords/stopwords.txt', 2)\n",
    "#corpus = Corpus(pres_speech_list, '/Users/ainalopez/Downloads/text_mining-master-2/data/stopwords/stopwords.txt', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################## HW3 ##########################\n",
    "###################################################\n",
    "\n",
    "\n",
    "\n",
    "#######################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define functions to get m_kv and n_dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# the number of words in document d that have topic allocation k\n",
    "def n_k(z, K):      \n",
    "    n = [sum(np.equal(z ,k)) for k in range(K)]\n",
    "    return n\n",
    "\n",
    "\n",
    "# for each word, the number of times each topic appears\n",
    "def m_k(Z, K): \n",
    "    m=[]\n",
    "    for k in range(K):\n",
    "        m.append([0]*N)\n",
    "    \n",
    "    for d in range(len(Z)):\n",
    "        print \"doc\" + str(d)\n",
    "        words = corpus.docs[d].tokens\n",
    "        for w in range(len(words)):\n",
    "            v=unique_words.index(words[w])\n",
    "            k=Z[d][w]\n",
    "            m[k][v]=m[k][v] + 1\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Start Algorithm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Theta updated!\n",
      "doc0\n",
      "doc1\n",
      "doc2\n",
      "doc3\n",
      "doc4\n",
      "doc5\n",
      "doc6\n",
      "doc7\n",
      "doc8\n",
      "doc9\n",
      "doc10\n",
      "doc11\n",
      "doc12\n",
      "doc13\n",
      "doc14\n",
      "doc15\n",
      "doc16\n",
      "doc17\n",
      "doc18\n",
      "doc19\n",
      "doc20\n",
      "doc21\n",
      "doc22\n",
      "doc23\n",
      "doc24\n",
      "doc25\n",
      "doc26\n",
      "doc27\n",
      "doc28\n",
      "doc29\n",
      "doc30\n",
      "doc31\n",
      "doc32\n",
      "doc33\n",
      "doc34\n",
      "doc35\n",
      "doc36\n",
      "doc37\n",
      "doc38\n",
      "doc39\n",
      "doc40\n",
      "doc41\n",
      "doc42\n",
      "doc43\n",
      "doc44\n",
      "doc45\n",
      "doc46\n",
      "doc47\n",
      "doc48\n",
      "doc49\n",
      "doc50\n",
      "doc51\n",
      "doc52\n",
      "doc53\n",
      "doc54\n",
      "doc55\n",
      "doc56\n",
      "doc57\n",
      "doc58\n",
      "doc59\n",
      "doc60\n",
      "doc61\n",
      "doc62\n",
      "doc63\n",
      "doc64\n",
      "doc65\n",
      "doc66\n",
      "doc67\n",
      "doc68\n",
      "doc69\n",
      "doc70\n",
      "doc71\n",
      "doc72\n",
      "doc73\n",
      "doc74\n",
      "doc75\n",
      "doc76\n",
      "doc77\n",
      "doc78\n",
      "doc79\n",
      "doc80\n",
      "doc81\n",
      "doc82\n",
      "doc83\n",
      "doc84\n",
      "doc85\n",
      "doc86\n",
      "doc87\n",
      "doc88\n",
      "doc89\n",
      "doc90\n",
      "doc91\n",
      "doc92\n",
      "doc93\n",
      "doc94\n",
      "doc95\n",
      "doc96\n",
      "doc97\n",
      "doc98\n",
      "doc99\n",
      "doc100\n",
      "doc101\n",
      "doc102\n",
      "doc103\n",
      "doc104\n",
      "doc105\n",
      "doc106\n",
      "doc107\n",
      "doc108\n",
      "doc109\n",
      "doc110\n",
      "doc111\n",
      "doc112\n",
      "doc113\n",
      "doc114\n",
      "doc115\n",
      "doc116\n",
      "doc117\n",
      "doc118\n",
      "doc119\n",
      "doc120\n",
      "doc121\n",
      "doc122\n",
      "doc123\n",
      "doc124\n",
      "doc125\n",
      "doc126\n",
      "doc127\n",
      "doc128\n",
      "doc129\n",
      "doc130\n",
      "doc131\n",
      "doc132\n",
      "doc133\n",
      "doc134\n",
      "doc135\n",
      "doc136\n",
      "doc137\n",
      "doc138\n",
      "doc139\n",
      "doc140\n",
      "doc141\n",
      "doc142\n",
      "doc143\n",
      "doc144\n",
      "doc145\n",
      "doc146\n",
      "doc147\n",
      "doc148\n",
      "doc149\n",
      "doc150\n",
      "doc151\n",
      "doc152\n",
      "doc153\n",
      "doc154\n",
      "doc155\n",
      "doc156\n",
      "doc157\n",
      "doc158\n",
      "doc159\n",
      "doc160\n",
      "doc161\n",
      "doc162\n",
      "doc163\n",
      "doc164\n",
      "doc165\n",
      "doc166\n",
      "doc167\n",
      "doc168\n",
      "doc169\n",
      "doc170\n",
      "doc171\n",
      "doc172\n",
      "doc173\n",
      "doc174\n",
      "doc175\n",
      "doc176\n",
      "doc177\n",
      "doc178\n",
      "doc179\n",
      "doc180\n",
      "doc181\n",
      "doc182\n",
      "doc183\n",
      "doc184\n",
      "doc185\n",
      "doc186\n",
      "doc187\n",
      "doc188\n",
      "doc189\n",
      "doc190\n",
      "doc191\n",
      "doc192\n",
      "doc193\n",
      "doc194\n",
      "doc195\n",
      "doc196\n",
      "doc197\n",
      "doc198\n",
      "doc199\n",
      "doc200\n",
      "doc201\n",
      "doc202\n",
      "doc203\n",
      "doc204\n",
      "doc205\n",
      "doc206\n",
      "doc207\n",
      "doc208\n",
      "doc209\n",
      "doc210\n",
      "doc211\n",
      "doc212\n",
      "doc213\n",
      "doc214\n",
      "doc215\n",
      "doc216\n",
      "doc217\n",
      "doc218\n",
      "doc219\n",
      "doc220\n",
      "doc221\n",
      "doc222\n",
      "doc223\n",
      "doc224\n",
      "doc225\n",
      "doc226\n",
      "doc227\n",
      "doc228\n",
      "doc229\n",
      "doc230\n",
      "doc231\n",
      "doc232\n",
      "doc233\n",
      "doc234\n",
      "doc235\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "object too deep for desired array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-162-d2722607b0aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0meta_new\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mbeta_cprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirichlet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meta_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mbeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirichlet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta_cprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Betas updated!\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.dirichlet (numpy/random/mtrand/mtrand.c:29593)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: object too deep for desired array"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "N = len(corpus.token_set) # number of unique words\n",
    "unique_words=list(corpus.token_set)\n",
    "D = corpus.N                  # number of documents\n",
    "K = 3                         # number of topics\n",
    "alpha = [1]*K             # hyperparameter 1\n",
    "eta = [1]*N                 # hyperparameter 2\n",
    "\n",
    "\n",
    "S = 5                         # number of iterations\n",
    "burn_in = 2                   # burn in period\n",
    "\n",
    "\n",
    "# num words in each document\n",
    "V =[len(corpus.docs[d].tokens) for d in range(D)]\n",
    "\n",
    "# Randomly allocate the topics list inside a list \n",
    "Z=[]\n",
    "for d in range(D):\n",
    "    new = [np.random.randint(K-1) for i in range(V[d])]\n",
    "    Z.append(new)\n",
    "    \n",
    "    \n",
    "# beta = matrix # % of topic in a word (given a document)\n",
    "random.seed(111)\n",
    "beta = np.random.dirichlet(eta, K)\n",
    "\n",
    "# theta = matrix col = topics # % of topic in a document \n",
    "random.seed(111)\n",
    "theta = np.random.dirichlet(alpha, D)\n",
    "\n",
    "\n",
    "# Gibbs Sampling algorithm\n",
    "\n",
    "for s in range(S):\n",
    "    # Allocation of topics to words\n",
    "    for d in range(10): \n",
    "\n",
    "        # Sample the assingnment of words\n",
    "        z = []\n",
    "        V = len(corpus.docs[d].tokens)\n",
    "\n",
    "\n",
    "        words = corpus.docs[d].tokens\n",
    "        V=[unique_words.index(x) for x in words]\n",
    "\n",
    "        for v in V:\n",
    "            # Compute probabilities\n",
    "            all_prob = [[theta[d,k] * beta[k,v] / np.dot(theta[d,],beta[:,v])] for k in (range(K))]\n",
    "            prob=[]\n",
    "            for x in range(K):\n",
    "                prob.append(all_prob[x][0])\n",
    "            prob=[x/3 for x in prob]\n",
    "\n",
    "            #Sample topics of words\n",
    "            z.append(np.argmax( np.random.multinomial(1, prob, 1) ))\n",
    "\n",
    "        Z[d]=z\n",
    "        print d\n",
    "    # Update Theta\n",
    "    for z in Z:\n",
    "        n_dk = n_k(z, K)\n",
    "        alpha_new=[(alpha[k] + n_dk[k]) for k in range(K)]\n",
    "        theta_cprobs = np.random.dirichlet(alpha_new, 1)[0]\n",
    "        theta[d] = np.random.dirichlet(theta_cprobs, 1)\n",
    "\n",
    "    print \"Theta updated!\"\n",
    "\n",
    "    # Update Betas\n",
    "    mk=m_k(Z,K)\n",
    "\n",
    "    for k in range(K):\n",
    "        eta_new=[(eta[n] + mk[k][n]) for n in range(N)]\n",
    "        beta_cprobs = np.random.dirichlet(eta_new, 1)[0]\n",
    "        beta[k] = np.random.dirichlet(beta_cprobs, 1)\n",
    "\n",
    "    print \"Betas updated!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.48210053e-06   1.43348322e-04   7.66228158e-06 ...,   3.14866592e-06\n",
      "   4.24934456e-05   8.61481981e-07]\n",
      "[  4.54462298e-06   1.61632525e-04   1.77190105e-05 ...,   8.49334695e-07\n",
      "   1.54036222e-05   2.56438766e-06]\n",
      "[  1.40336203e-05   1.77720286e-06   1.24278422e-05 ...,   6.15729584e-05\n",
      "   1.31087434e-04   1.44956312e-05]\n",
      "Betas updated!\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
