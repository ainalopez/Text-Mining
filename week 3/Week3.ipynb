{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Homework\n",
    "\n",
    "Group *May oh ness*\n",
    "\n",
    "Nick Halliwell, Aina Lopez, Yaroslav Marchuk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A skeleton class structure for documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "import math \n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from itertools import repeat\n",
    "\n",
    "\n",
    "class Corpus():\n",
    "    \n",
    "    \"\"\" \n",
    "    The Corpus class represents a document collection\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, doc_data, stopword_file, clean_length):\n",
    "        \"\"\"\n",
    "        Notice that the __init__ method is invoked everytime an object of the class\n",
    "        is instantiated\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        #Initialise documents by invoking the appropriate class\n",
    "        self.docs = [Document(doc[0], doc[1], doc[2]) for doc in doc_data] \n",
    "        \n",
    "        self.N = len(self.docs)\n",
    "        self.clean_length = clean_length\n",
    "        \n",
    "        #get a list of stopwords\n",
    "        self.create_stopwords(stopword_file, clean_length)\n",
    "        \n",
    "        #stopword removal, token cleaning and stemming to docs\n",
    "        self.clean_docs(2)\n",
    "        \n",
    "        #create vocabulary\n",
    "        self.corpus_tokens()\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \"\"\" \n",
    "        Applies stopword removal, token cleaning and stemming to docs\n",
    "        \"\"\"\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            doc.stem()        \n",
    "    \n",
    "    def create_stopwords(self, stopword_file, length):\n",
    "        \"\"\"\n",
    "        description: parses a file of stowords, removes words of length 'length' and \n",
    "        stems it\n",
    "        input: length: cutoff length for words\n",
    "               stopword_file: stopwords file to parse\n",
    "        \"\"\"\n",
    "        \n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        \n",
    "        self.stopwords = (np.array([PorterStemmer().stem(word) \n",
    "                                    for word in list(raw.splitlines()) if len(word) > length]))\n",
    "        \n",
    "     \n",
    "    def corpus_tokens(self):\n",
    "        \"\"\"\n",
    "        description: create a set of all all tokens or in other words a vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        #initialise an empty set\n",
    "        self.token_set = set()\n",
    "        for doc in self.docs:\n",
    "            self.token_set = self.token_set.union(doc.tokens) \n",
    "            \n",
    "    def document_term_matrix(self):\n",
    "        \"\"\"\n",
    "        description:  returns a D by V array of frequency counts\n",
    "        \"\"\"  \n",
    "        # subroutine: computes the counts of each vocabulary in the document\n",
    "        def counts(doc):\n",
    "            # initialize a matrix\n",
    "            term_mat = [0]*len(self.token_set)\n",
    "            for token in doc.tokens:\n",
    "                term_mat[list(self.token_set).index(token)] = term_mat[list(self.token_set).index(token)] + 1\n",
    "            return term_mat;\n",
    "            \n",
    "        self.doc_term_matrix = []\n",
    "        \n",
    "        for doc in self.docs:\n",
    "            self.doc_term_matrix.append([doc.pres + \" \" + doc.year, counts(doc)])\n",
    "\n",
    "\n",
    "      \n",
    "    def tf_idf(self):\n",
    "        \"\"\"\n",
    "        description:  returns a D by V array of tf-idf scores\n",
    "        \"\"\"\n",
    "        # Compute inverse document frequency \n",
    "        idf = [0]*len(self.token_set)\n",
    "        for token in self.token_set:\n",
    "            ind = 0\n",
    "            for doc in self.docs:\n",
    "                if token in doc.tokens:\n",
    "                    ind += 1 \n",
    "            idf[list(self.token_set).index(token)] = math.log(self.N/ind)\n",
    "        \n",
    "        # Create a subroutine that computes tf_idf for one document\n",
    "        def tfidf(doc):\n",
    "            term_mat = [0]*len(self.token_set)\n",
    "            for token in doc.tokens:\n",
    "                term_mat[list(self.token_set).index(token)] = term_mat[list(self.token_set).index(token)] + 1 \n",
    "        \n",
    "            for i,term in enumerate(term_mat):\n",
    "                if term != 0:\n",
    "                    term_mat[i] = (1 + math.log(term)) * idf[i]\n",
    "            return term_mat;\n",
    "        \n",
    "        #tf_idf\n",
    "        self.tf_idf_matrix = []\n",
    "        for doc in self.docs:\n",
    "            self.tf_idf_matrix.append([doc.pres + \" \" + doc.year, tfidf(doc)])\n",
    "            \n",
    "            \n",
    "        \n",
    "    def dict_rank(self, n, dictionary, token_repr):\n",
    "        \"\"\"\n",
    "        description:  returns the top n documents based on a given dictionary and represenation of tokens\n",
    "        \"\"\"\n",
    "        if token_repr == \"tf-idf\":\n",
    "            self.tf_idf()\n",
    "            representation = self.tf_idf_matrix\n",
    "            \n",
    "        if token_repr == \"doc-term\":\n",
    "            self.document_term_matrix()\n",
    "            representation = self.doc_term_matrix\n",
    "            \n",
    "        # Return top n docs based on dictionary given\n",
    "        score = []\n",
    "        x=self.token_set\n",
    "        x=list(x)\n",
    "        for token in x: \n",
    "            try:\n",
    "                score.append(dictionary[token])\n",
    "            except: \n",
    "                score.append(0)\n",
    "\n",
    "        # get a vector with all the scores in order\n",
    "        score=[int(x) for x in score]\n",
    "        rank = {}\n",
    "        elements=range(len(representation))\n",
    "   \n",
    "        for i in elements:\n",
    "            rank[representation[i][0]] = np.dot(score,representation[i][1])\n",
    "            \n",
    "        # Get sorted view of the keys.\n",
    "        s = sorted(rank, key=rank.get, reverse=True)[0:(n-1)]\n",
    "        \n",
    "        ranking = {}\n",
    "        for key in s:\n",
    "            ranking[key] =  rank[key]\n",
    "        \n",
    "        return ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Document():\n",
    "    \n",
    "    \"\"\" The Doc class rpresents a class of individul documents\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, speech_year, speech_pres, speech_text):\n",
    "        self.year = speech_year\n",
    "        self.pres = speech_pres\n",
    "        self.text = speech_text.lower()\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        description: strip out non-alpha tokens and tokens of length > 'length'\n",
    "        input: length: cut off length \n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Remove stopwords from tokens.\n",
    "        input: stopwords: a suitable list of stopwords\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.tokens = np.array([PorterStemmer().stem(t) for t in self.tokens])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Load the document and create the corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_text(textraw, regex):\n",
    "    \"\"\"takes raw string and performs two operations\n",
    "    1. Breaks text into a list of speech, president and speech\n",
    "    2. breaks speech into paragraphs\n",
    "    \"\"\"\n",
    "    prs_yr_spch_reg = re.compile(regex, re.MULTILINE|re.DOTALL)\n",
    "    \n",
    "    #Each tuple contains the year, last ane of the president and the speech text\n",
    "    prs_yr_spch = prs_yr_spch_reg.findall(textraw)\n",
    "    \n",
    "    #convert immutabe tuple to mutable list\n",
    "    prs_yr_spch = [list(tup) for tup in prs_yr_spch]\n",
    "    \n",
    "    for i in range(len(prs_yr_spch)):\n",
    "        prs_yr_spch[i][2] = prs_yr_spch[i][2].replace('\\n', '')\n",
    "    \n",
    "    #sort\n",
    "    prs_yr_spch.sort()\n",
    "    \n",
    "    return(prs_yr_spch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text = open(\"/Users/ainalopez/Downloads/text_mining-master/data/pres_speech/sou_all copy.txt\", 'r').read()\n",
    "text = open(\"/home/yaroslav/Projects/text_mining/data/pres_speech/sou_all.txt\", 'r').read()\n",
    "regex = \"_(\\d{4}).*?_[a-zA-Z]+.*?_[a-zA-Z]+.*?_([a-zA-Z]+)_\\*+(\\\\n{2}.*?)\\\\n{3}\"\n",
    "pres_speech_list = parse_text(text, regex)\n",
    "pres_speech_list=pres_speech_list[0:10]\n",
    "\n",
    "#Instantite the corpus class\n",
    "corpus = Corpus(pres_speech_list, '/home/yaroslav/Projects/text_mining/data/stopwords/stopwords.txt', 2)\n",
    "#corpus = Corpus(pres_speech_list, '/Users/ainalopez/Downloads/text_mining-master-2/data/stopwords/stopwords.txt', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define functions to get m_kv and n_dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# the number of words in document d that have topic allocation k\n",
    "def n_k(z, K):      \n",
    "    n = [sum(np.equal(z ,k)) for k in range(K)]\n",
    "    return n\n",
    "\n",
    "\n",
    "# for each word, the number of times each topic appears\n",
    "def m_k(Z, K): \n",
    "    m=[]\n",
    "    for k in range(K):\n",
    "        m.append([0]*N)\n",
    "    \n",
    "    for d in range(len(Z)):\n",
    "        #print \"doc\" + str(d)\n",
    "        words = corpus.docs[d].tokens\n",
    "        for w in range(len(words)):\n",
    "            v=unique_words.index(words[w])\n",
    "            k=Z[d][w]\n",
    "            m[k][v]=m[k][v] + 1\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Start Algorithm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S = 0\n",
      "S = 1\n",
      "S = 2\n",
      "S = 3\n",
      "S = 4\n",
      "LDA Successfully Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaroslav/Programs/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:48: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "N = len(corpus.token_set)     # number of unique words\n",
    "unique_words=list(corpus.token_set)\n",
    "D = corpus.N                  # number of documents\n",
    "K = 3                         # number of topics\n",
    "alpha = [1]*K                 # hyperparameter 1\n",
    "eta = [1]*N                   # hyperparameter 2\n",
    "\n",
    "\n",
    "S = 5                         # number of iterations\n",
    "burn_in = 2                   # burn in period\n",
    "\n",
    "# num words in each document\n",
    "V =[len(corpus.docs[d].tokens) for d in range(D)]\n",
    "\n",
    "# Randomly allocate the topics list inside a list \n",
    "Z=[]\n",
    "for d in range(D):\n",
    "    new = [np.random.randint(K-1) for i in range(V[d])]\n",
    "    Z.append(new)\n",
    "       \n",
    "# beta = matrix # % of topic in a word (given a document)\n",
    "random.seed(111)\n",
    "beta = np.random.dirichlet(eta, K)\n",
    "beta_general = np.zeros((K,N))\n",
    "\n",
    "# theta = matrix col = topics # % of topic in a document \n",
    "random.seed(111)\n",
    "theta = np.random.dirichlet(alpha, D)\n",
    "theta_general = np.zeros((D,K))\n",
    "\n",
    "# Gibbs Sampling algorithm\n",
    "\n",
    "for s in range(S):\n",
    "    print \"S = \" + str(s)\n",
    "    # Allocation of topics to words\n",
    "    for d in range(D): \n",
    "        # Sample the assingnment of words\n",
    "        z = []\n",
    "        words = corpus.docs[d].tokens\n",
    "        V=[unique_words.index(x) for x in words]\n",
    "\n",
    "        for v in V: \n",
    "            # Compute probabilities\n",
    "            all_prob = [theta[d,k] * beta[k,v] / np.dot(theta[d,],beta[:,v]) for k in (range(K))]\n",
    "            prob_sum = sum(all_prob)\n",
    "            prob = [x/prob_sum for x in all_prob]\n",
    "\n",
    "            #Sample topics of words\n",
    "            z.append(np.argmax( np.random.multinomial(1, prob, 1) ))\n",
    "\n",
    "        Z[d]=z\n",
    "        #print d\n",
    "        \n",
    "    # Update Theta\n",
    "    for z in Z:\n",
    "        n_dk = n_k(z, K)\n",
    "        alpha_new=[(alpha[k] + n_dk[k]) for k in range(K)]\n",
    "        theta_cprobs = np.random.dirichlet(alpha_new, 1)[0]\n",
    "        theta[d] = np.random.dirichlet(theta_cprobs, 1)\n",
    "\n",
    "    #print \"Theta updated!\"\n",
    "\n",
    "    # Update Betas\n",
    "    mk=m_k(Z,K)\n",
    "\n",
    "    for k in range(K):\n",
    "        eta_new=[(eta[n] + mk[k][n]) for n in range(N)]\n",
    "        beta_cprobs = np.random.dirichlet(eta_new, 1)[0]\n",
    "        beta[k] = np.random.dirichlet(beta_cprobs, 1)\n",
    "\n",
    "    #print \"Betas updated!\"\n",
    "    \n",
    "    if(s >= burn_in):\n",
    "   \n",
    "        # store beta\n",
    "        for i in range(len(beta_general)):\n",
    "            for j in range(len(beta_general[i])):\n",
    "                beta_general[i][j]=beta_general[i][j]+beta[i][j]\n",
    "\n",
    "\n",
    "        # store theta\n",
    "        for i in range(len(theta_general)):\n",
    "            for j in range(len(theta_general[i])):\n",
    "                theta_general[i][j]=theta_general[i][j]+theta[i][j]\n",
    "        \n",
    "# divide by n\n",
    "for i in range(len(beta_general)):\n",
    "    for j in range(len(beta_general[i])):\n",
    "        beta_general[i][j]=beta_general[i][j]/(S - burn_in)\n",
    "        \n",
    "#divide beta\n",
    "\n",
    "for i in range(len(theta_general)):\n",
    "    for j in range(len(theta_general[i])):\n",
    "        theta_general[i][j]=theta_general[i][j]/(S - burn_in)\n",
    "        \n",
    "print \"LDA Successfully Completed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2062\n",
      "3\n",
      "0.0\n",
      "_Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "print len(beta_general[0])\n",
    "print S-burn_in\n",
    "print beta_general[2][8]/(S-burn_in)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
