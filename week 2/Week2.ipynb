{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A skeleton class structure for documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "import math \n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from itertools import repeat\n",
    "\n",
    "\n",
    "class Corpus():\n",
    "    \n",
    "    \"\"\" \n",
    "    The Corpus class represents a document collection\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, doc_data, stopword_file, clean_length):\n",
    "        \"\"\"\n",
    "        Notice that the __init__ method is invoked everytime an object of the class\n",
    "        is instantiated\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        #Initialise documents by invoking the appropriate class\n",
    "        self.docs = [Document(doc[0], doc[1], doc[2]) for doc in doc_data] \n",
    "        \n",
    "        self.N = len(self.docs)\n",
    "        self.clean_length = clean_length\n",
    "        \n",
    "        #get a list of stopwords\n",
    "        self.create_stopwords(stopword_file, clean_length)\n",
    "        \n",
    "        #stopword removal, token cleaning and stemming to docs\n",
    "        self.clean_docs(2)\n",
    "        \n",
    "        #create vocabulary\n",
    "        self.corpus_tokens()\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \"\"\" \n",
    "        Applies stopword removal, token cleaning and stemming to docs\n",
    "        \"\"\"\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            doc.stem()        \n",
    "    \n",
    "    def create_stopwords(self, stopword_file, length):\n",
    "        \"\"\"\n",
    "        description: parses a file of stowords, removes words of length 'length' and \n",
    "        stems it\n",
    "        input: length: cutoff length for words\n",
    "               stopword_file: stopwords file to parse\n",
    "        \"\"\"\n",
    "        \n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        \n",
    "        self.stopwords = (np.array([PorterStemmer().stem(word) \n",
    "                                    for word in list(raw.splitlines()) if len(word) > length]))\n",
    "        \n",
    "     \n",
    "    def corpus_tokens(self):\n",
    "        \"\"\"\n",
    "        description: create a set of all all tokens or in other words a vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        #initialise an empty set\n",
    "        self.token_set = set()\n",
    "        for doc in self.docs:\n",
    "            self.token_set = self.token_set.union(doc.tokens) \n",
    "            \n",
    "    def document_term_matrix(self):\n",
    "        \"\"\"\n",
    "        description:  returns a D by V array of frequency counts\n",
    "        \"\"\"  \n",
    "        # subroutine: computes the counts of each vocabulary in the document\n",
    "        def counts(doc):\n",
    "            # initialize a matrix\n",
    "            term_mat = [0]*len(self.token_set)\n",
    "            for token in doc.tokens:\n",
    "                term_mat[list(self.token_set).index(token)] = term_mat[list(self.token_set).index(token)] + 1\n",
    "            return term_mat;\n",
    "            \n",
    "        self.doc_term_matrix = []\n",
    "        \n",
    "        for doc in self.docs:\n",
    "            self.doc_term_matrix.append([doc.pres + \" \" + doc.year, counts(doc)])\n",
    "\n",
    "\n",
    "      \n",
    "    def tf_idf(self):\n",
    "        \"\"\"\n",
    "        description:  returns a D by V array of tf-idf scores\n",
    "        \"\"\"\n",
    "        # Compute inverse document frequency \n",
    "        idf = [0]*len(self.token_set)\n",
    "        for token in self.token_set:\n",
    "            ind = 0\n",
    "            for doc in self.docs:\n",
    "                if token in doc.tokens:\n",
    "                    ind += 1 \n",
    "            idf[list(self.token_set).index(token)] = math.log(self.N/ind)\n",
    "        \n",
    "        # Create a subroutine that computes tf_idf for one document\n",
    "        def tfidf(doc):\n",
    "            term_mat = [0]*len(self.token_set)\n",
    "            for token in doc.tokens:\n",
    "                term_mat[list(self.token_set).index(token)] = term_mat[list(self.token_set).index(token)] + 1 \n",
    "        \n",
    "            for i,term in enumerate(term_mat):\n",
    "                if term != 0:\n",
    "                    term_mat[i] = (1 + math.log(term)) * idf[i]\n",
    "            return term_mat;\n",
    "        \n",
    "        #tf_idf\n",
    "        self.tf_idf_matrix = []\n",
    "        for doc in self.docs:\n",
    "            self.tf_idf_matrix.append([doc.pres + \" \" + doc.year, tfidf(doc)])\n",
    "            \n",
    "            \n",
    "        \n",
    "    def dict_rank(self, n, dictionary, token_repr):\n",
    "        \"\"\"\n",
    "        description:  returns the top n documents based on a given dictionary and represenation of tokens\n",
    "        \"\"\"\n",
    "        if token_repr == \"tf-idf\":\n",
    "            self.tf_idf()\n",
    "            representation = self.tf_idf_matrix\n",
    "            \n",
    "        if token_repr == \"doc-term\":\n",
    "            self.document_term_matrix()\n",
    "            representation = self.doc_term_matrix\n",
    "            \n",
    "        # Return top n docs based on dictionary given\n",
    "        score = []\n",
    "        x=self.token_set\n",
    "        x=list(x)\n",
    "        for token in x: \n",
    "            try:\n",
    "                score.append(dictionary[token])\n",
    "            except: \n",
    "                score.append(0)\n",
    "\n",
    "        # get a vector with all the scores in order\n",
    "        score=[int(x) for x in score]\n",
    "        rank = {}\n",
    "        elements=range(len(representation))\n",
    "   \n",
    "        for i in elements:\n",
    "            rank[representation[i][0]] = np.dot(score,representation[i][1])\n",
    "            \n",
    "        # Get sorted view of the keys.\n",
    "        s = sorted(rank, key=rank.get, reverse=True)[0:(n-1)]\n",
    "        \n",
    "        ranking = {}\n",
    "        for key in s:\n",
    "            ranking[key] =  rank[key]\n",
    "        \n",
    "        return ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Document():\n",
    "    \n",
    "    \"\"\" The Doc class rpresents a class of individul documents\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, speech_year, speech_pres, speech_text):\n",
    "        self.year = speech_year\n",
    "        self.pres = speech_pres\n",
    "        self.text = speech_text.lower()\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        description: strip out non-alpha tokens and tokens of length > 'length'\n",
    "        input: length: cut off length \n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Remove stopwords from tokens.\n",
    "        input: stopwords: a suitable list of stopwords\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.tokens = np.array([PorterStemmer().stem(t) for t in self.tokens])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Load the document and create the corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_text(textraw, regex):\n",
    "    \"\"\"takes raw string and performs two operations\n",
    "    1. Breaks text into a list of speech, president and speech\n",
    "    2. breaks speech into paragraphs\n",
    "    \"\"\"\n",
    "    prs_yr_spch_reg = re.compile(regex, re.MULTILINE|re.DOTALL)\n",
    "    \n",
    "    #Each tuple contains the year, last ane of the president and the speech text\n",
    "    prs_yr_spch = prs_yr_spch_reg.findall(textraw)\n",
    "    \n",
    "    #convert immutabe tuple to mutable list\n",
    "    prs_yr_spch = [list(tup) for tup in prs_yr_spch]\n",
    "    \n",
    "    for i in range(len(prs_yr_spch)):\n",
    "        prs_yr_spch[i][2] = prs_yr_spch[i][2].replace('\\n', '')\n",
    "    \n",
    "    #sort\n",
    "    prs_yr_spch.sort()\n",
    "    \n",
    "    return(prs_yr_spch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = open(\"/Users/ainalopez/Downloads/text_mining-master/data/pres_speech/sou_all copy.txt\", 'r').read()\n",
    "# text = open(\"/home/yaroslav/Projects/text_mining/data/pres_speech/sou_all.txt\", 'r').read()\n",
    "regex = \"_(\\d{4}).*?_[a-zA-Z]+.*?_[a-zA-Z]+.*?_([a-zA-Z]+)_\\*+(\\\\n{2}.*?)\\\\n{3}\"\n",
    "pres_speech_list = parse_text(text, regex)\n",
    "\n",
    "#Instantite the corpus class\n",
    "# corpus = Corpus(pres_speech_list, '/home/yaroslav/Projects/text_mining/data/stopwords/stopwords.txt', 2)\n",
    "corpus = Corpus(pres_speech_list, '/Users/ainalopez/Downloads/text_mining-master-2/data/stopwords/stopwords.txt', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. tf_idf matrix and SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute tf_idf matrix \n",
    "corpus.tf_idf()\n",
    "matrix = corpus.tf_idf_matrix\n",
    "\n",
    "X =[] \n",
    "presidents = []\n",
    "\n",
    "for row in matrix:\n",
    "    X.append(row[1])  #= #np.append(X, row[1], axis=0) \n",
    "    presidents.append(row[0])\n",
    "\n",
    "X = np.array(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n",
      "236\n"
     ]
    }
   ],
   "source": [
    "# Compute svd \n",
    "sing_values_nb = 200\n",
    " \n",
    "U, s, V = np.linalg.svd(X)\n",
    "X_hat = np.dot(U[:,0:(sing_values_nb-1)] * s[0:(sing_values_nb-1)], V[0:(sing_values_nb-1),:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Cosine Similarity of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(doc1, doc2):\n",
    "    return np.dot(doc1, doc2) / ( math.sqrt(np.dot(doc1, doc1))* math.sqrt(np.dot(doc2, doc2)) )\n",
    "\n",
    "similarity_X = np.zeros((len(X), len(X)))\n",
    "similarity_X_hat = np.zeros((len(X_hat), len(X_hat)))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    for j in range(len(X)):\n",
    "        similarity_X[i][j] = cosine_similarity(X[i], X[j])\n",
    "        \n",
    "for i in range(len(X_hat)):\n",
    "    for j in range(len(X_hat)):\n",
    "        similarity_X_hat[i][j] = cosine_similarity(X_hat[i], X_hat[j])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.00000000e+00   7.49326794e-01   7.51415662e-01   7.69472364e-01\n",
      "   7.90452706e-01   4.34195597e-01   7.80456758e-01   4.89702452e-01\n",
      "   5.48881075e-01   4.79137936e-01   6.71030648e-01   6.36541279e-01\n",
      "   6.13853717e-01   5.27436790e-01   6.30542818e-01   5.91960182e-01\n",
      "   4.72158937e-01   4.70753323e-01   6.68473408e-01   7.20662281e-01\n",
      "   6.25009698e-01   5.41988987e-01   5.69118430e-01   3.64309390e-01\n",
      "   3.67186655e-01   3.27858307e-01   6.17343439e-01   6.67380166e-01\n",
      "   5.01802466e-01   1.34750177e-01   3.27550343e-01   3.71668487e-01\n",
      "   1.81623111e-01   3.20056234e-01   1.61827790e-01   3.12842859e-01\n",
      "   3.06853137e-01   2.09709611e-01   1.27639730e-01   1.95065406e-01\n",
      "   2.07997270e-01   3.26363602e-01   2.50856805e-01   3.35286591e-01\n",
      "   2.24071340e-01   3.17540865e-01   3.51993151e-01   3.90887320e-01\n",
      "   3.25197211e-01   3.29905384e-01   3.73251469e-01   2.75478603e-01\n",
      "   2.18282261e-01   1.92925939e-01   2.76055469e-01   2.57769437e-01\n",
      "   2.28380882e-01   2.39454558e-01   3.13653231e-01   2.14118729e-01\n",
      "   1.52186493e-01   2.92495582e-01   2.60438834e-01   2.25854554e-01\n",
      "   2.61117696e-01   3.31378875e-01   1.98883805e-01   1.86756580e-01\n",
      "   2.39992926e-01   2.54954774e-01   2.80906810e-01   2.13627119e-01\n",
      "   2.34966059e-01   1.73737586e-01   2.02621957e-01   2.35013528e-01\n",
      "   1.94623829e-01   1.17079458e-01   1.96675635e-01   2.00743510e-01\n",
      "   1.72547030e-01   1.71717632e-01   1.95373746e-01   1.77518399e-01\n",
      "   1.29960118e-01   2.18896410e-01   1.69733854e-01   1.63071761e-01\n",
      "   2.60484117e-01   1.87790518e-01   2.52006931e-01   2.17669087e-01\n",
      "   2.11703812e-01   1.31036240e-01   1.45565991e-01   1.71409674e-01\n",
      "   2.22117657e-01   2.14557721e-01   1.16007659e-01   1.79642647e-01\n",
      "   1.85986938e-01   1.41218544e-01   1.51815408e-01   1.41281182e-01\n",
      "   1.67255037e-01   1.84087533e-01   2.22294687e-01   2.05153237e-01\n",
      "   1.40433828e-01   1.81532224e-01   1.88083191e-01   1.71219786e-01\n",
      "   1.76704762e-01   1.64414169e-01   1.81355244e-01   1.78854773e-01\n",
      "   1.33110387e-01   1.54361178e-01   1.73545970e-01   1.28268541e-01\n",
      "   1.17758446e-01   1.70854846e-01   1.55710982e-01   1.82007719e-01\n",
      "   1.05946977e-01   1.08162619e-01   1.33948024e-01   2.94281365e-01\n",
      "   6.92363122e-02   1.21053640e-01   4.23600246e-02   2.53055239e-01\n",
      "   1.14704628e-01   8.27702821e-02   1.24263322e-01   8.62810315e-02\n",
      "   1.00700241e-01   1.43091704e-01   1.22425317e-01   8.15235596e-02\n",
      "   8.51779549e-02   4.25428940e-02   8.42587717e-02   1.58757699e-01\n",
      "   1.30166353e-01   4.43187832e-02   1.09785911e-01   9.44058885e-02\n",
      "   5.64377808e-02   3.99980821e-02   2.95896015e-02   8.41995606e-02\n",
      "   4.90509189e-02   4.45827666e-02   7.17199473e-02   1.02543700e-01\n",
      "   1.28482871e-01   1.06271507e-01   4.31706157e-02   1.13636016e-01\n",
      "   8.37139163e-02   4.92510958e-02   2.04944049e-03   4.17633103e-02\n",
      "   1.45468286e-01   5.90758842e-02   1.04279864e-01   1.01175877e-01\n",
      "   8.35842883e-02   1.39840869e-01   3.57894172e-02   7.70110331e-02\n",
      "   9.34189063e-02   9.95283909e-02   6.78502275e-02   3.55871363e-02\n",
      "   1.09763462e-01   4.29701164e-02   9.27850483e-02   2.06630380e-01\n",
      "   1.19025358e-01   7.14977436e-02   7.73533107e-02   3.69514108e-02\n",
      "   1.09477761e-01   3.84421835e-02   6.61497270e-02   1.02574001e-01\n",
      "   1.08552300e-01   8.99884161e-02   7.53508614e-02   1.06794717e-01\n",
      "   5.85874878e-02   1.21386649e-01   4.53685894e-02   1.10834453e-01\n",
      "   6.32169935e-02   7.82166248e-04   8.77931680e-02   8.52950007e-02\n",
      "   8.38756928e-02   1.02595752e-01   4.77460492e-02   5.45408047e-02\n",
      "   5.45614672e-02   8.15935141e-02   4.25855964e-02   5.29491292e-02\n",
      "   1.17829322e-01   8.84985877e-02   4.97530671e-02   2.68258299e-02\n",
      "   2.40540548e-02   4.86070702e-02   6.46298662e-02   5.08155783e-02\n",
      "   1.09012229e-01   5.73010884e-02   4.19248192e-02   7.32230138e-02\n",
      "   6.78958446e-02   7.19352449e-02   3.08092366e-02   5.00563715e-02\n",
      "   6.51019005e-02   5.07187318e-02   8.96121687e-02   8.35756982e-02\n",
      "   1.03250422e-01   1.22841799e-01   3.51958120e-02   5.01080209e-02\n",
      "   4.83294368e-02   4.11315859e-02   5.09323193e-02   8.76374447e-02]\n"
     ]
    }
   ],
   "source": [
    "# Add labels per president \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
