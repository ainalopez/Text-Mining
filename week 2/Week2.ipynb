{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Homework\n",
    "\n",
    "Group *May oh ness*\n",
    "\n",
    "Nick Halliwell, Aina Lopez, Yaroslav Marchuk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A skeleton class structure for documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "import math \n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from itertools import repeat\n",
    "\n",
    "\n",
    "class Corpus():\n",
    "    \n",
    "    \"\"\" \n",
    "    The Corpus class represents a document collection\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, doc_data, stopword_file, clean_length):\n",
    "        \"\"\"\n",
    "        Notice that the __init__ method is invoked everytime an object of the class\n",
    "        is instantiated\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        #Initialise documents by invoking the appropriate class\n",
    "        self.docs = [Document(doc[0], doc[1], doc[2]) for doc in doc_data] \n",
    "        \n",
    "        self.N = len(self.docs)\n",
    "        self.clean_length = clean_length\n",
    "        \n",
    "        #get a list of stopwords\n",
    "        self.create_stopwords(stopword_file, clean_length)\n",
    "        \n",
    "        #stopword removal, token cleaning and stemming to docs\n",
    "        self.clean_docs(2)\n",
    "        \n",
    "        #create vocabulary\n",
    "        self.corpus_tokens()\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \"\"\" \n",
    "        Applies stopword removal, token cleaning and stemming to docs\n",
    "        \"\"\"\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            doc.stem()        \n",
    "    \n",
    "    def create_stopwords(self, stopword_file, length):\n",
    "        \"\"\"\n",
    "        description: parses a file of stowords, removes words of length 'length' and \n",
    "        stems it\n",
    "        input: length: cutoff length for words\n",
    "               stopword_file: stopwords file to parse\n",
    "        \"\"\"\n",
    "        \n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        \n",
    "        self.stopwords = (np.array([PorterStemmer().stem(word) \n",
    "                                    for word in list(raw.splitlines()) if len(word) > length]))\n",
    "        \n",
    "     \n",
    "    def corpus_tokens(self):\n",
    "        \"\"\"\n",
    "        description: create a set of all all tokens or in other words a vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        #initialise an empty set\n",
    "        self.token_set = set()\n",
    "        for doc in self.docs:\n",
    "            self.token_set = self.token_set.union(doc.tokens) \n",
    "            \n",
    "    def document_term_matrix(self):\n",
    "        \"\"\"\n",
    "        description:  returns a D by V array of frequency counts\n",
    "        \"\"\"  \n",
    "        # subroutine: computes the counts of each vocabulary in the document\n",
    "        def counts(doc):\n",
    "            # initialize a matrix\n",
    "            term_mat = [0]*len(self.token_set)\n",
    "            for token in doc.tokens:\n",
    "                term_mat[list(self.token_set).index(token)] = term_mat[list(self.token_set).index(token)] + 1\n",
    "            return term_mat;\n",
    "            \n",
    "        self.doc_term_matrix = []\n",
    "        \n",
    "        for doc in self.docs:\n",
    "            self.doc_term_matrix.append([doc.pres + \" \" + doc.year, counts(doc)])\n",
    "\n",
    "\n",
    "      \n",
    "    def tf_idf(self):\n",
    "        \"\"\"\n",
    "        description:  returns a D by V array of tf-idf scores\n",
    "        \"\"\"\n",
    "        # Compute inverse document frequency \n",
    "        idf = [0]*len(self.token_set)\n",
    "        for token in self.token_set:\n",
    "            ind = 0\n",
    "            for doc in self.docs:\n",
    "                if token in doc.tokens:\n",
    "                    ind += 1 \n",
    "            idf[list(self.token_set).index(token)] = math.log(self.N/ind)\n",
    "        \n",
    "        # Create a subroutine that computes tf_idf for one document\n",
    "        def tfidf(doc):\n",
    "            term_mat = [0]*len(self.token_set)\n",
    "            for token in doc.tokens:\n",
    "                term_mat[list(self.token_set).index(token)] = term_mat[list(self.token_set).index(token)] + 1 \n",
    "        \n",
    "            for i,term in enumerate(term_mat):\n",
    "                if term != 0:\n",
    "                    term_mat[i] = (1 + math.log(term)) * idf[i]\n",
    "            return term_mat;\n",
    "        \n",
    "        #tf_idf\n",
    "        self.tf_idf_matrix = []\n",
    "        for doc in self.docs:\n",
    "            self.tf_idf_matrix.append([doc.pres + \" \" + doc.year, tfidf(doc)])\n",
    "            \n",
    "            \n",
    "        \n",
    "    def dict_rank(self, n, dictionary, token_repr):\n",
    "        \"\"\"\n",
    "        description:  returns the top n documents based on a given dictionary and represenation of tokens\n",
    "        \"\"\"\n",
    "        if token_repr == \"tf-idf\":\n",
    "            self.tf_idf()\n",
    "            representation = self.tf_idf_matrix\n",
    "            \n",
    "        if token_repr == \"doc-term\":\n",
    "            self.document_term_matrix()\n",
    "            representation = self.doc_term_matrix\n",
    "            \n",
    "        # Return top n docs based on dictionary given\n",
    "        score = []\n",
    "        x=self.token_set\n",
    "        x=list(x)\n",
    "        for token in x: \n",
    "            try:\n",
    "                score.append(dictionary[token])\n",
    "            except: \n",
    "                score.append(0)\n",
    "\n",
    "        # get a vector with all the scores in order\n",
    "        score=[int(x) for x in score]\n",
    "        rank = {}\n",
    "        elements=range(len(representation))\n",
    "   \n",
    "        for i in elements:\n",
    "            rank[representation[i][0]] = np.dot(score,representation[i][1])\n",
    "            \n",
    "        # Get sorted view of the keys.\n",
    "        s = sorted(rank, key=rank.get, reverse=True)[0:(n-1)]\n",
    "        \n",
    "        ranking = {}\n",
    "        for key in s:\n",
    "            ranking[key] =  rank[key]\n",
    "        \n",
    "        return ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Document():\n",
    "    \n",
    "    \"\"\" The Doc class rpresents a class of individul documents\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, speech_year, speech_pres, speech_text):\n",
    "        self.year = speech_year\n",
    "        self.pres = speech_pres\n",
    "        self.text = speech_text.lower()\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        description: strip out non-alpha tokens and tokens of length > 'length'\n",
    "        input: length: cut off length \n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Remove stopwords from tokens.\n",
    "        input: stopwords: a suitable list of stopwords\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.tokens = np.array([PorterStemmer().stem(t) for t in self.tokens])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Load the document and create the corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_text(textraw, regex):\n",
    "    \"\"\"takes raw string and performs two operations\n",
    "    1. Breaks text into a list of speech, president and speech\n",
    "    2. breaks speech into paragraphs\n",
    "    \"\"\"\n",
    "    prs_yr_spch_reg = re.compile(regex, re.MULTILINE|re.DOTALL)\n",
    "    \n",
    "    #Each tuple contains the year, last ane of the president and the speech text\n",
    "    prs_yr_spch = prs_yr_spch_reg.findall(textraw)\n",
    "    \n",
    "    #convert immutabe tuple to mutable list\n",
    "    prs_yr_spch = [list(tup) for tup in prs_yr_spch]\n",
    "    \n",
    "    for i in range(len(prs_yr_spch)):\n",
    "        prs_yr_spch[i][2] = prs_yr_spch[i][2].replace('\\n', '')\n",
    "    \n",
    "    #sort\n",
    "    prs_yr_spch.sort()\n",
    "    \n",
    "    return(prs_yr_spch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = open(\"/Users/ainalopez/Downloads/text_mining-master/data/pres_speech/sou_all copy.txt\", 'r').read()\n",
    "# text = open(\"/home/yaroslav/Projects/text_mining/data/pres_speech/sou_all.txt\", 'r').read()\n",
    "regex = \"_(\\d{4}).*?_[a-zA-Z]+.*?_[a-zA-Z]+.*?_([a-zA-Z]+)_\\*+(\\\\n{2}.*?)\\\\n{3}\"\n",
    "pres_speech_list = parse_text(text, regex)\n",
    "\n",
    "#Instantite the corpus class\n",
    "# corpus = Corpus(pres_speech_list, '/home/yaroslav/Projects/text_mining/data/stopwords/stopwords.txt', 2)\n",
    "corpus = Corpus(pres_speech_list, '/Users/ainalopez/Downloads/text_mining-master-2/data/stopwords/stopwords.txt', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. tf_idf matrix and SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute tf_idf matrix \n",
    "corpus.tf_idf()\n",
    "matrix = corpus.tf_idf_matrix\n",
    "\n",
    "X =[] \n",
    "presidents = []\n",
    "\n",
    "for row in matrix:\n",
    "    X.append(row[1])  \n",
    "    presidents.append(row[0].split()[0])\n",
    "\n",
    "X = np.array(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute svd \n",
    "sing_values_nb = 70\n",
    " \n",
    "U, s, V = np.linalg.svd(X)\n",
    "X_hat = np.dot(U[:,0:(sing_values_nb-1)] * s[0:(sing_values_nb-1)], V[0:(sing_values_nb-1),:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Cosine Similarity of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(doc1, doc2):\n",
    "    return np.dot(doc1, doc2) / ( math.sqrt(np.dot(doc1, doc1))* math.sqrt(np.dot(doc2, doc2)) )\n",
    "\n",
    "similarity_X = np.zeros((len(X), len(X)))\n",
    "similarity_X_hat = np.zeros((len(X_hat), len(X_hat)))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    for j in range(len(X)):\n",
    "        similarity_X[i][j] = cosine_similarity(X[i], X[j])\n",
    "        \n",
    "for i in range(len(X_hat)):\n",
    "    for j in range(len(X_hat)):\n",
    "        similarity_X_hat[i][j] = cosine_similarity(X_hat[i], X_hat[j])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create labels for the speeches made by Republicans and Democrats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add labels per president \n",
    "Republicans = [\"Lincoln\", \"Grant\", \"Hayes\", \"Eisenhower\", \"Arthur\", \"Harrison\", \"McKinley\", \n",
    "              \"Taft\", \"Harding\", \"Coolidge\", \"Hoover\", \"Nixon\", \"Ford\", \"Reagan\", \"Bush\", \"Tyler\", \"Taylor\", \"Fillmore\"]\n",
    "\n",
    "Democrats = [\"Jackson\", \"Buren\", \"Polk\", \"Pierce\", \"Buchanan\", \"Johnson\", \"Cleveland\", \"Wilson\", \n",
    "             \"Truman\", \"Kennedy\", \"Carter\", \"Clinton\", \"Obama\"]\n",
    "\n",
    "Others = [\"Jefferson\", \"Madison\", \"Monroe\", \"Adams\" , \"Washington\"]\n",
    "\n",
    "\n",
    "ideology = []\n",
    "R = []\n",
    "D = []\n",
    "\n",
    "for i in range(len(presidents)):\n",
    "    if presidents[i] in Republican:\n",
    "        ideology.append('Republicans')\n",
    "        R.append(i)\n",
    "    elif presidents[i] in Democratic:\n",
    "        ideology.append('Democrats')\n",
    "        D.append(i)\n",
    "    elif presidents[i] in Others:\n",
    "        ideology.append('Others')\n",
    "    else:\n",
    "        ideology.append('NA')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since similarity matrices are very large, we cannot inspect them directly. \n",
    "\n",
    "We are going to compute the mean of the similarity of Democrat speeches, the mean of similarity of Republican speeches and the mean of similarity between Republican and democrat speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Republican speeches \n",
      "Without SVD:  0.0838146250777\n",
      "With SVD:  0.279057416427\n",
      " \n",
      "Democrat speeches \n",
      "Without SVD:  0.0893912241585\n",
      "With SVD:  0.30031307907\n",
      " \n",
      "Others speeches \n",
      "Without SVD:  0.0849360505636\n",
      "With SVD:  0.285312456328\n"
     ]
    }
   ],
   "source": [
    "rep_rep = []\n",
    "dem_dem = []\n",
    "rep_dem = []\n",
    "\n",
    "rep_rep_h = []\n",
    "rep_dem_h = []\n",
    "dem_dem_h = []\n",
    "\n",
    "\n",
    "for r in range(len(R)):\n",
    "    rep_rep.extend(similarity_X[r, R])\n",
    "    rep_rep_h.extend(similarity_X_hat[r, R])\n",
    "    rep_dem.extend(similarity_X[r, D])\n",
    "    rep_dem_h.extend(similarity_X_hat[r, D])\n",
    "    \n",
    "for d in range(len(D)):\n",
    "    dem_dem.extend(similarity_X[d, D])\n",
    "    dem_dem_h.extend(similarity_X_hat[d, D])\n",
    "    rep_dem.extend(similarity_X[d, R])\n",
    "    rep_dem_h.extend(similarity_X_hat[d, R])\n",
    "\n",
    "\n",
    "print \"Republican speeches \"\n",
    "print \"Without SVD: \", np.mean(rep_rep)\n",
    "print \"With SVD: \", np.mean(rep_rep_h)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print \"Democrat speeches \"\n",
    "print \"Without SVD: \",np.mean(dem_dem) \n",
    "print \"With SVD: \",np.mean(dem_dem_h)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print \"Others speeches \"\n",
    "print \"Without SVD: \", np.mean(rep_dem)\n",
    "print \"With SVD: \", np.mean(rep_dem_h)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "Republican speeches similarity\n",
    "Without SVD:  0.0838146250777\n",
    "With SVD:  0.279057416427\n",
    " \n",
    "Democrat speeches similarity\n",
    "Without SVD:  0.0893912241585\n",
    "With SVD:  0.30031307907\n",
    " \n",
    "Democrat vs Republican speeches similarity\n",
    "Without SVD:  0.0849360505636\n",
    "With SVD:  0.285312456328\n",
    "\n",
    "We can see that the average cosine similarity within speeches of the same ideology increased when we used SVD. However, surprisingly, the average cosine similarity within speeches of different ideology also increased.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
