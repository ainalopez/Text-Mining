{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Oriented Programming - Quick and dirty Introduction\n",
    "\n",
    "In this session, we go through a toy example of object oriented programming in python and then look at the skeleton class of a document. Clearly, this topic is vast and requires a much more through treatment than being given here. But I find it more useful to introduce you to concepts because we need them rather than because they exist. \n",
    "\n",
    "#### Why will we use object oriented programming in this class?\n",
    "Amongst the many many reasons that OOP makes sense, for us the most important one is:\n",
    "\n",
    "1. __Reusability__\n",
    "We want to be able to write all our code for every document once and then call it again and again. Having stand alone functinons not only increases the amount of _house keeping_ but will also require you to change code at different places everytime your requirements change.\n",
    "\n",
    "2. __Encapsulation__\n",
    "Remember that you will have hundreds of documents and will have to define properties for each one of them. For example, how long is every document or what are its tokens. OOP allows you to define all of these properties at one place and then the details of these implementations can be hidden.\n",
    "\n",
    "#### Important Terminology\n",
    "\n",
    "1. __Class__: Is _dna_ or _blueprint_ for an object that has to be modeled. Contains attributes that contins its properties and methods that characterize its behaviors.\n",
    "\n",
    "2. __Instance__: An individual object of a class.\n",
    "\n",
    "3. __Instantiation__: The creation of an instance of a class.\n",
    "\n",
    "4. __Object__: A unique instance of a class. An object comprises both data members (class variables and instance variables) and methods.\n",
    "\n",
    "5. __Class variable__: A variable that is shared by all instances of a class. They are defined within a class but outside any of the class's methods\n",
    "\n",
    "6. __Instance variable__: A variable that is defined inside a method and belongs only to the current instance of a class.\n",
    "\n",
    "7. __Data member__: A class variable or instance variable that holds data associated with a class and its objects.\n",
    "\n",
    "8. __Method__ : A function that is defined in a class definition. \n",
    "\n",
    "So all the chit chat aside lets get down to it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "import re\n",
    "\n",
    "\"\"\"\n",
    "This is a class sherlock. \n",
    "Notice how it is defined with the keyword `class` and a name that begins with a capital letter\n",
    "\"\"\"\n",
    "class Document():\n",
    "    \n",
    "    \"\"\" The Doc class rpresents a class of individul documents\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, speech_year, speech_pres, speech_text):\n",
    "        \"\"\"\n",
    "        The __init__ method is called everytime an object is instantiated.\n",
    "        This is where you will define all the properties of the object that it must have\n",
    "        when it is `born`.\n",
    "        \"\"\"\n",
    "        \n",
    "        #These are data members\n",
    "        self.year = speech_year\n",
    "        self.pres = speech_pres\n",
    "        self.text = speech_text.lower()\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        description: strip out non-alpha tokens and tokens of length > 'length'\n",
    "        input: length: cut off length \n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Remove stopwords from tokens.\n",
    "        input: stopwords: a suitable list of stopwords\n",
    "        \"\"\"\n",
    " \n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.tokens = np.array([PorterStemmer().stem(t) for t in self.tokens])\n",
    "        \n",
    "    def demo_self():\n",
    "        print 'this will error out'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self\n",
    "Notice the `self` keyword that is present all over the place in the class. `self` tells python which object it needs to work with. \n",
    "\n",
    "1. Therefore every method should have a `self` parameter specified.\n",
    "2. Notice that when the method is called the reference to the object is passed implicitly.\n",
    "3. Every data member of the class needs to be referred to using `self`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Document instance at 0x7f354d389128>\n",
      "['this' 'is' 'a' 'chicken']\n",
      "demo_self() takes no arguments (1 given)\n"
     ]
    }
   ],
   "source": [
    "#Instantiating an object.  \n",
    "speech1 = Document('1986', 'Rooster', 'this is a chicken')\n",
    "print speech1\n",
    "\n",
    "#Accessing data members\n",
    "print speech1.tokens\n",
    "\n",
    "try:\n",
    "    speech1.demo_self()\n",
    "except Exception as ex:\n",
    "    print ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of usefulness of classes\n",
    "\n",
    "Use this [link](http://www.codeskulptor.org/#user41_X9owzYCGupNiKYr.py) to see how useful classes can be. The link leads to a code I wrote two years ago that implements a very naive version of the game Asteroids. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A skeleton class structure for documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "import math \n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from itertools import repeat\n",
    "\n",
    "\n",
    "class Corpus():\n",
    "    \n",
    "    \"\"\" \n",
    "    The Corpus class represents a document collection\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, doc_data, stopword_file, clean_length):\n",
    "        \"\"\"\n",
    "        Notice that the __init__ method is invoked everytime an object of the class\n",
    "        is instantiated\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        #Initialise documents by invoking the appropriate class\n",
    "        self.docs = [Document(doc[0], doc[1], doc[2]) for doc in doc_data] \n",
    "        \n",
    "        self.N = len(self.docs)\n",
    "        self.clean_length = clean_length\n",
    "        \n",
    "        #get a list of stopwords\n",
    "        self.create_stopwords(stopword_file, clean_length)\n",
    "        \n",
    "        #stopword removal, token cleaning and stemming to docs\n",
    "        self.clean_docs(2)\n",
    "        \n",
    "        #create vocabulary\n",
    "        self.corpus_tokens()\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \"\"\" \n",
    "        Applies stopword removal, token cleaning and stemming to docs\n",
    "        \"\"\"\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            doc.stem()        \n",
    "    \n",
    "    def create_stopwords(self, stopword_file, length):\n",
    "        \"\"\"\n",
    "        description: parses a file of stowords, removes words of length 'length' and \n",
    "        stems it\n",
    "        input: length: cutoff length for words\n",
    "               stopword_file: stopwords file to parse\n",
    "        \"\"\"\n",
    "        \n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        \n",
    "        self.stopwords = (np.array([PorterStemmer().stem(word) \n",
    "                                    for word in list(raw.splitlines()) if len(word) > length]))\n",
    "        \n",
    "     \n",
    "    def corpus_tokens(self):\n",
    "        \"\"\"\n",
    "        description: create a set of all all tokens or in other words a vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        #initialise an empty set\n",
    "        self.token_set = set()\n",
    "        for doc in self.docs:\n",
    "            self.token_set = self.token_set.union(doc.tokens) \n",
    "            \n",
    "    def document_term_matrix(self):\n",
    "        \"\"\"\n",
    "        description:  returns a D by V array of frequency counts\n",
    "        \"\"\"  \n",
    "        # subroutine: computes the counts of each vocabulary in the document\n",
    "        def counts(doc):\n",
    "            # initialize a matrix\n",
    "            term_mat = [0]*len(self.token_set)\n",
    "            for token in doc.tokens:\n",
    "                term_mat[list(self.token_set).index(token)] = term_mat[list(self.token_set).index(token)] + 1\n",
    "            return term_mat;\n",
    "            \n",
    "        self.doc_term_matrix = []\n",
    "        \n",
    "        for doc in self.docs:\n",
    "            self.doc_term_matrix.append(counts(doc))\n",
    "\n",
    "\n",
    "      \n",
    "    def tf_idf(self):\n",
    "        \"\"\"\n",
    "        description:  returns a D by V array of tf-idf scores\n",
    "        \"\"\"\n",
    "        # Compute inverse document frequency \n",
    "        idf = [0]*len(self.token_set)\n",
    "        for token in self.token_set:\n",
    "            ind = 0\n",
    "            for doc in self.docs:\n",
    "                if token in doc.tokens:\n",
    "                    ind += 1 \n",
    "            idf[list(self.token_set).index(token)] = math.log(self.N/ind)\n",
    "        \n",
    "        # Create a subroutine that computes tf_idf for one document\n",
    "        def tfidf(doc):\n",
    "            term_mat = [0]*len(self.token_set)\n",
    "            for token in doc.tokens:\n",
    "                term_mat[list(self.token_set).index(token)] = term_mat[list(self.token_set).index(token)] + 1 \n",
    "        \n",
    "            for i,term in enumerate(term_mat):\n",
    "                if term != 0:\n",
    "                    term_mat[i] = (1 + math.log(term)) * idf[i]\n",
    "            return term_mat;\n",
    "        \n",
    "        #tf_idf\n",
    "        self.tf_idf_matrix = []\n",
    "        for doc in self.docs:\n",
    "            self.tf_idf_matrix.append(tfidf(doc))\n",
    "            \n",
    "            \n",
    "        \n",
    "    def dict_rank(self, n, dictionary, token_repr):\n",
    "        \"\"\"\n",
    "        description:  returns the top n documents based on a given dictionary and represenation of tokens\n",
    "        \"\"\"\n",
    "        if token_repr == \"tf-idf\":\n",
    "            self.tf_idf()\n",
    "            representation = self.tf_idf_matrix\n",
    "            \n",
    "        if token_repr == \"doc-term\":\n",
    "            self.document_term_matrix()\n",
    "            representation = self.doc_term_matrix\n",
    "            \n",
    "        # Return top n docs based on dictionary given\n",
    "        \n",
    "        score = []\n",
    "        x=self.token_set\n",
    "        x=list(x)\n",
    "        for token in x: \n",
    "            try:\n",
    "                score.append(dictionary[token])\n",
    "            except: \n",
    "                score.append(0)\n",
    "\n",
    "        # get a vector with all the scores in order\n",
    "        score=[int(x) for x in score]\n",
    "        rank = {}\n",
    "        elements=range(len(representation))\n",
    "        for i in elements:\n",
    "            rank[\"DOC-\"+str(i)] = np.dot(score,representation[i])\n",
    "        \n",
    "            \n",
    "            \n",
    "        # Get sorted view of the keys.\n",
    "        s = sorted(rank, key=rank.get, reverse=True)[0:(n-1)]\n",
    "        \n",
    "        ranking = {}\n",
    "        for key in s:\n",
    "            ranking[key] =  rank[key]\n",
    "        \n",
    "        return ranking \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Document():\n",
    "    \n",
    "    \"\"\" The Doc class rpresents a class of individul documents\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, speech_year, speech_pres, speech_text):\n",
    "        self.year = speech_year\n",
    "        self.pres = speech_pres\n",
    "        self.text = speech_text.lower()\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        description: strip out non-alpha tokens and tokens of length > 'length'\n",
    "        input: length: cut off length \n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Remove stopwords from tokens.\n",
    "        input: stopwords: a suitable list of stopwords\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.tokens = np.array([PorterStemmer().stem(t) for t in self.tokens])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the presedential speech dataset to demonstrate how the class works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_text(textraw, regex):\n",
    "    \"\"\"takes raw string and performs two operations\n",
    "    1. Breaks text into a list of speech, president and speech\n",
    "    2. breaks speech into paragraphs\n",
    "    \"\"\"\n",
    "    prs_yr_spch_reg = re.compile(regex, re.MULTILINE|re.DOTALL)\n",
    "    \n",
    "    #Each tuple contains the year, last ane of the president and the speech text\n",
    "    prs_yr_spch = prs_yr_spch_reg.findall(textraw)\n",
    "    \n",
    "    #convert immutabe tuple to mutable list\n",
    "    prs_yr_spch = [list(tup) for tup in prs_yr_spch]\n",
    "    \n",
    "    for i in range(len(prs_yr_spch)):\n",
    "        prs_yr_spch[i][2] = prs_yr_spch[i][2].replace('\\n', '')\n",
    "    \n",
    "    #sort\n",
    "    prs_yr_spch.sort()\n",
    "    \n",
    "    return(prs_yr_spch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#text = open(\"/Users/ainalopez/Downloads/text_mining-master/data/pres_speech/sou_all.txt\", 'r').read()\n",
    "text = open(\"/home/yaroslav/Projects/text_mining/data/pres_speech/sou_all.txt\", 'r').read()\n",
    "regex = \"_(\\d{4}).*?_[a-zA-Z]+.*?_[a-zA-Z]+.*?_([a-zA-Z]+)_\\*+(\\\\n{2}.*?)\\\\n{3}\"\n",
    "pres_speech_list = parse_text(text, regex)\n",
    "# Select only some documents\n",
    "pres_speech_list = pres_speech_list[230:235]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Instantite the corpus class\n",
    "corpus = Corpus(pres_speech_list, '/home/yaroslav/Projects/text_mining/data/stopwords/stopwords.txt', 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import dictionary from excel file\n",
    "import pandas as pd\n",
    "df = pd.read_excel(\"/home/yaroslav/Dropbox/BGSE/3rd Term/Text Mining/LoughranMcDonald_MasterDictionary_2014.xlsx\", skiprows=0)\n",
    "w = df['Word']\n",
    "words = [str(x).lower() for x in df['Word']]\n",
    "words=[PorterStemmer().stem(t) for t in words]\n",
    "score = [str(x).lower() for x in df['Positive']] # or any other method\n",
    "dictionary=dict(zip(words,score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Applying dict_rank function\n",
    "X1 = corpus.dict_rank(10, dictionary,\"doc-term\")\n",
    "X2 = corpus.dict_rank(10, dictionary,\"tf-idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DOC-4': 136612, 'DOC-0': 130585, 'DOC-1': 136612, 'DOC-2': 122549, 'DOC-3': 94423}\n",
      "{'DOC-4': 24026.058048305684, 'DOC-0': 16554.292251414543, 'DOC-1': 13885.554901304156, 'DOC-2': 19325.300257919924, 'DOC-3': 9251.7869036500761}\n"
     ]
    }
   ],
   "source": [
    "print X1\n",
    "print X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[136612, 130585, 136612, 122549, 94423]\n",
      "           score1\n",
      "president        \n",
      "Obama      124156\n",
      "                 score1\n",
      "president              \n",
      "Obama      16608.598473\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2:\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "# Get list of presidents\n",
    "presidents = [presi.pres for presi in corpus.docs]\n",
    "pr1 = list(X1.keys())\n",
    "pr2 = list(X2.keys())\n",
    "\n",
    "pr1=[x[4:] for x in pr1]\n",
    "pr2=[x[4:] for x in pr2]\n",
    "\n",
    "pr1=[int(x) for x in pr1]\n",
    "pr2=[int(x) for x in pr2]\n",
    "\n",
    "pres1 = [presidents[i] for i in pr1]\n",
    "pres2 = [presidents[i] for i in pr2]\n",
    "\n",
    "scores1 = list(X1.values())\n",
    "scores2 = list(X2.values())\n",
    "print scores1\n",
    "\n",
    "df1 = DataFrame({\n",
    "        'president': pres1,\n",
    "        'score1': scores1\n",
    "        })\n",
    "\n",
    "df2 = DataFrame({\n",
    "        'president': pres2,\n",
    "        'score1': scores2\n",
    "        })\n",
    "\n",
    "\n",
    "g1=df1.groupby(['president']).mean()\n",
    "g2=df2.groupby(['president']).mean()\n",
    "#g = df1.groupby('president').mean().sort_values(ascending=False)\n",
    "#most_rated = lens.groupby('title').size().sort_values(ascending=False)[:25]\n",
    "print g1\n",
    "print g2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 0, 1, 2, 3]\n",
      "['Obama', 'Obama', 'Obama', 'Obama', 'Obama']\n"
     ]
    }
   ],
   "source": [
    "pr1 = list(X1.keys())\n",
    "pr2 = list(X2.keys())\n",
    "\n",
    "pr1=[x[4:] for x in pr1]\n",
    "pr2=[x[4:] for x in pr2]\n",
    "\n",
    "pr1=[int(x) for x in pr1]\n",
    "pr2=[int(x) for x in pr2]\n",
    "\n",
    "presidents = [presi.pres for presi in corpus.docs]\n",
    "pres1 = [presidents[i] for i in pr1]\n",
    "pres2 = [presidents[i] for i in pr2]\n",
    "\n",
    "print pres1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
